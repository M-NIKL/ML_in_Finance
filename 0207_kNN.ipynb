{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$ Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of Nearest Neighbors\n",
    "\n",
    "Arguably one of the simplest classification algorithm is $k$ nearest neighbors (KNN). Below figure portrays the algorithm's simplicity. We see a training set of five red and five blue dots, representing some label 1 and 0, respectively. The two axes represent two features, e.g. income and credit card balance. If we now add a new test data point $x_0$ (green dot), KNN will label this test data according to its $k$ closest neighbors. In below figure we set $k=3$. The three closest neighbors are: one blue dot and two red dots, resulting in estimated probabilities of 2/3 for the red class and 1/3 for the blue class. Hence KNN will predict that the new data point will belong to the red class. More so, based on the training set the algorithm is able to draw a decision boundary. (Note: In a classification settings with $j$ classes a *decision boundary* is a hyperplane that partitions the underlying vector space into $j$ sets, one for each class.). This is shown with the jagged line that separates background colors cyan (blue label) and light blue (red label). Given any possible pair of feature values, KNN labels the response along the drawn decision boundary. With $k=1$ the boundary line is very jagged. Increasing the number of $k$ will smoothen the decision boundary. This tells us that small values of $k$ will produce large variance but low bias, meaning that each new added training point might change the decision boundary line significantly but the decision boundary separates the training set (almost) correctly. As $k$ increases, variance decreases but bias increases. This is a manifestation of the Bias-Variance Trade-Off as discussed in the script ([Fortmann-Roe (2012)](http://scott.fortmann-roe.com/docs/BiasVariance.html)) and highlights the importance of selecting an adequate value of $k$ - a topic we will pick up in a future chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/0207_KNN.png\" alt=\"LinearRegClassification\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Description of KNN\n",
    "\n",
    "Having introduced KNN illustratively, let us now define this in mathematical terms. Let our data set be $\\{(x_1, y_1), (x_2, y_2), \\ldots (x_n, y_n)\\}$ with $x_i \\in \\mathbb{R}^p$ and $y_i \\in \\{0, 1\\}\\; \\forall i \\in \\{1, 2, \\ldots, n\\}$. Based on the $k$ neighbors, KNN estimates the conditional probability for class $j$ as the fraction of points in $\\mathcal{N}(k, x_0)$ (the set of the $k$ closest neighbors of $x_0$) whose response values equals $j$ (Russell and Norvig (2009)):\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\Pr(Y = j | X = x_0) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}(k, x_0)} \\mathbb{I}(y_i = j).\n",
    "\\end{equation}$$\n",
    "\n",
    "Once the probability for each class $j$ is calculated, the  KNN classifier predicts a class label $\\hat{y}_0$ for the new data point $x_0$ by maximizing the conditional probability (Batista and Silva (2009)).\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\hat{y}_0 = \\arg \\max_{j} \\frac{1}{k} \\sum_{i \\in \\mathcal{N}(k,x_0)} \\mathbb{I}(y_i = j)\n",
    "\\end{equation}$$\n",
    "\n",
    "Selecting a new data point $x_0$'s nearest neighbors requires some notion of **distance measure**. Most researchers chose Minkowski's distance, which is often referred to as $L^m$ norm (Guggenbuehler (2015)). The distance between points $x_a$ and $x_b$ in $\\mathbb{R}^p$ is then defined as follows (Russell and Norvig (2009)):\n",
    "\n",
    "$$\\begin{equation}\n",
    "L^m (x_a, x_b) = \\left(\\sum_{i=1}^p |x_{a, i} - x_{b, i}|^m \\right)^{1/m}\n",
    "\\end{equation}$$\n",
    "\n",
    "Using $m=2$, above equation simplifies to the well known Euclidean distance and $m=1$ yields the Manhattten distance. Python's `sklearn` package, short for scikit-learn, offers [several other options](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html) that we will not discuss. \n",
    "\n",
    "Beyond the pure distance measure, it is also possible to weight training data points relative to their distance from a certain point. In above figure distance is weighted uniformly. Alternatively one could weight points by the inverse of their distance (closer neighbors of a query point will have a greater influence than neighbors which are further away) or any other user-defined weighting function. For [further details check `sklearn`'s documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=neighbors%20kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier) for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Predicting Share Price Movement\n",
    "\n",
    "### Loading Data\n",
    "\n",
    "The application of KNN is shown using simple stock market data. The idea is to predict a stock's movement based on simple features such as:\n",
    "* `Lag1, Lag2`: log returns of the two previous trading days \n",
    "* `SMI`: SMI log return of the previous day\n",
    "\n",
    "The response is a binary variable: if a stock closed above the previous day's closing price it equals 1, and 0 if it fell. We start by loading the necessary packages and stock data from a csv - a procedure we are well acquainted with by now. Thus comments are held short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABBN</th>\n",
       "      <th>ADEN</th>\n",
       "      <th>BAER</th>\n",
       "      <th>CFR</th>\n",
       "      <th>CSGN</th>\n",
       "      <th>GEBN</th>\n",
       "      <th>GIVN</th>\n",
       "      <th>LHN</th>\n",
       "      <th>LONN</th>\n",
       "      <th>NESN</th>\n",
       "      <th>...</th>\n",
       "      <th>ROG</th>\n",
       "      <th>SCMN</th>\n",
       "      <th>SGSN</th>\n",
       "      <th>SLHN</th>\n",
       "      <th>SREN</th>\n",
       "      <th>UBSG</th>\n",
       "      <th>UHR</th>\n",
       "      <th>ZURN</th>\n",
       "      <th>SIK</th>\n",
       "      <th>SMI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-06-26</th>\n",
       "      <td>24.63</td>\n",
       "      <td>73.85</td>\n",
       "      <td>51.35</td>\n",
       "      <td>80.45</td>\n",
       "      <td>13.49</td>\n",
       "      <td>457.0</td>\n",
       "      <td>1947.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>208.2</td>\n",
       "      <td>85.65</td>\n",
       "      <td>...</td>\n",
       "      <td>251.7</td>\n",
       "      <td>469.3</td>\n",
       "      <td>2375.0</td>\n",
       "      <td>323.9</td>\n",
       "      <td>89.40</td>\n",
       "      <td>15.71</td>\n",
       "      <td>367.5</td>\n",
       "      <td>284.4</td>\n",
       "      <td>6310.0</td>\n",
       "      <td>9121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-27</th>\n",
       "      <td>24.40</td>\n",
       "      <td>73.70</td>\n",
       "      <td>51.65</td>\n",
       "      <td>80.20</td>\n",
       "      <td>13.74</td>\n",
       "      <td>453.8</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>56.40</td>\n",
       "      <td>207.6</td>\n",
       "      <td>84.30</td>\n",
       "      <td>...</td>\n",
       "      <td>252.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>2355.0</td>\n",
       "      <td>322.7</td>\n",
       "      <td>88.80</td>\n",
       "      <td>16.00</td>\n",
       "      <td>362.0</td>\n",
       "      <td>281.7</td>\n",
       "      <td>6280.0</td>\n",
       "      <td>9072.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-28</th>\n",
       "      <td>24.07</td>\n",
       "      <td>73.55</td>\n",
       "      <td>51.65</td>\n",
       "      <td>80.00</td>\n",
       "      <td>13.82</td>\n",
       "      <td>452.5</td>\n",
       "      <td>1927.0</td>\n",
       "      <td>56.35</td>\n",
       "      <td>207.5</td>\n",
       "      <td>85.40</td>\n",
       "      <td>...</td>\n",
       "      <td>251.2</td>\n",
       "      <td>464.9</td>\n",
       "      <td>2342.0</td>\n",
       "      <td>325.3</td>\n",
       "      <td>88.55</td>\n",
       "      <td>16.29</td>\n",
       "      <td>360.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>6195.0</td>\n",
       "      <td>9076.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-29</th>\n",
       "      <td>23.63</td>\n",
       "      <td>72.85</td>\n",
       "      <td>51.30</td>\n",
       "      <td>78.60</td>\n",
       "      <td>14.10</td>\n",
       "      <td>446.0</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>55.00</td>\n",
       "      <td>204.7</td>\n",
       "      <td>83.65</td>\n",
       "      <td>...</td>\n",
       "      <td>246.9</td>\n",
       "      <td>463.9</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>323.3</td>\n",
       "      <td>87.80</td>\n",
       "      <td>16.36</td>\n",
       "      <td>353.4</td>\n",
       "      <td>276.3</td>\n",
       "      <td>6145.0</td>\n",
       "      <td>8944.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>23.68</td>\n",
       "      <td>72.90</td>\n",
       "      <td>50.45</td>\n",
       "      <td>79.00</td>\n",
       "      <td>13.86</td>\n",
       "      <td>447.2</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>54.90</td>\n",
       "      <td>207.3</td>\n",
       "      <td>83.45</td>\n",
       "      <td>...</td>\n",
       "      <td>244.2</td>\n",
       "      <td>462.7</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>323.6</td>\n",
       "      <td>87.65</td>\n",
       "      <td>16.24</td>\n",
       "      <td>354.1</td>\n",
       "      <td>279.1</td>\n",
       "      <td>6160.0</td>\n",
       "      <td>8906.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ABBN   ADEN   BAER    CFR   CSGN   GEBN    GIVN    LHN   LONN  \\\n",
       "Date                                                                         \n",
       "2017-06-26  24.63  73.85  51.35  80.45  13.49  457.0  1947.0  56.25  208.2   \n",
       "2017-06-27  24.40  73.70  51.65  80.20  13.74  453.8  1931.0  56.40  207.6   \n",
       "2017-06-28  24.07  73.55  51.65  80.00  13.82  452.5  1927.0  56.35  207.5   \n",
       "2017-06-29  23.63  72.85  51.30  78.60  14.10  446.0  1913.0  55.00  204.7   \n",
       "2017-06-30  23.68  72.90  50.45  79.00  13.86  447.2  1918.0  54.90  207.3   \n",
       "\n",
       "             NESN  ...    ROG   SCMN    SGSN   SLHN   SREN   UBSG    UHR  \\\n",
       "Date               ...                                                     \n",
       "2017-06-26  85.65  ...  251.7  469.3  2375.0  323.9  89.40  15.71  367.5   \n",
       "2017-06-27  84.30  ...  252.0  465.0  2355.0  322.7  88.80  16.00  362.0   \n",
       "2017-06-28  85.40  ...  251.2  464.9  2342.0  325.3  88.55  16.29  360.0   \n",
       "2017-06-29  83.65  ...  246.9  463.9  2310.0  323.3  87.80  16.36  353.4   \n",
       "2017-06-30  83.45  ...  244.2  462.7  2322.0  323.6  87.65  16.24  354.1   \n",
       "\n",
       "             ZURN     SIK      SMI  \n",
       "Date                                \n",
       "2017-06-26  284.4  6310.0  9121.22  \n",
       "2017-06-27  281.7  6280.0  9072.92  \n",
       "2017-06-28  278.0  6195.0  9076.73  \n",
       "2017-06-29  276.3  6145.0  8944.04  \n",
       "2017-06-30  279.1  6160.0  8906.89  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import daily shares prices and select window\n",
    "shsPr = pd.read_csv('Data/SMIDataDaily.csv', sep=',',\n",
    "                     parse_dates=['Date'], dayfirst=True,\n",
    "                     index_col=['Date'])\n",
    "shsPr = shsPr['2012-06-01':'2017-06-30']\n",
    "shsPr = shsPr.sort_index()\n",
    "shsPr.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the data in a proper dataframe, we are now in a position to create the features and response values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log-returns and label responses: \n",
    "# 'direction' equals 1 if stock closed above \n",
    "# previous day and 0 if it fell.\n",
    "today = np.log(shsPr / shsPr.shift(1))\n",
    "direction = np.where(today >= 0, 1, 0)\n",
    "\n",
    "# Convert 'direction' to dataframe\n",
    "direction = pd.DataFrame(direction, index=today.index, columns=today.columns)\n",
    "\n",
    "# Lag1, 2: t-1 and t-2 returns; excl. smi (in last column)\n",
    "Lag1 = np.log(shsPr.iloc[:, :-1].shift(1) / shsPr.iloc[:, :-1].shift(2))\n",
    "Lag2 = np.log(shsPr.iloc[:, :-1].shift(2) / shsPr.iloc[:, :-1].shift(3))\n",
    "\n",
    "# Previous day return for SMI index\n",
    "smi  = np.log(shsPr.iloc[:, -1].shift(1) / shsPr.iloc[:, -1].shift(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Algorithm Applied\n",
    "\n",
    "Now comes the difficult part. What we want to achieve is to run the KNN algorithm for every stock and for different hyperparameter $k$ and see how it performs. For this we do the following steps:\n",
    "\n",
    "1. Create a feature matrix `X` containing `Lag1`, `Lag2` and `SMI` data for share i\n",
    "2. Create a response vector `y` with binary direction values\n",
    "3. Split data to training (before 2016-06-30) and test set (after 2016-06-30)\n",
    "4. Run KNN for different values of $k$ (loop)\n",
    "5. Write test score for given $k$ to matrix `scr`\n",
    "6. Once we have run through all $k$'s we proceed with step 1. with share i+1\n",
    "\n",
    "This means we need two loops. The first corresponds to the share (e.g. ABB, Adecco, etc.), the second runs the KNN algorithm for different values of $k$. \n",
    "\n",
    "The reason for this approach is that we are interested in finding any pattern/structure that would provide a successful trading strategy. There is obviously no free lunch. Predicting share price direction is by no means an easy task and we must be well aware that we are in for a difficult job here. If it were simple, neither one of us would be sitting here but run his own fund. But nonetheless, let us see how KNN performs and how homogeneous (or heterogeneous) the results are.\n",
    "\n",
    "As usual our first step is to prepare the ground by loading the necessary package and defining some auxiliary variables. The KNN function we will be using is available through the `sklearn` (short for Scikit-learn) package. We only load the `neighbor` sublibrary which contains the needed KNN function called `KNeigborsClassifier()`. KNN is applied with the default distance metric: Euclidean distance (Minkowski's distance with $m=2$). If we would prefer another distance metric we would have to specify it ([see documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant functions\n",
    "from sklearn import neighbors\n",
    "\n",
    "# k = {1, 3, ..., 200}\n",
    "k = np.arange(1, 200, 2)\n",
    "\n",
    "# Array to store results in. Dimension is [k x m] \n",
    "# with m=20 for the 20 companies (excl. SMI)\n",
    "scr = np.empty(shape=(len(k), len(shsPr.columns)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(shsPr.columns)-1):\n",
    "    \n",
    "    # 1) Create matrix with feature values of stock i\n",
    "    X = pd.concat([Lag1.iloc[:, i], Lag2.iloc[:, i], smi], axis=1)\n",
    "    X = X[3:]  # Drop first three rows with NaN (due to lag)\n",
    "    \n",
    "    # 2) Remove first three rows of response dataframe\n",
    "    #    to have equal no. of rows for features and response\n",
    "    y = direction.iloc[:, i]\n",
    "    y = y[3:]\n",
    "    \n",
    "    # 3) Split data into training set...\n",
    "    X_train = X[:'2016-06-30']\n",
    "    y_train = y[:'2016-06-30']\n",
    "    # ...and test set.\n",
    "    X_test = X['2016-07-01':]\n",
    "    y_test = y['2016-07-01':]\n",
    "    \n",
    "    # Convert responses to 1xN array (with .ravel() function)\n",
    "    y_train = y_train.values.ravel()\n",
    "    y_test  = y_test.values.ravel()\n",
    "    \n",
    "    for j in range(len(k)):\n",
    "        \n",
    "        # 4) Run KNN\n",
    "        # Instantiate KNN class\n",
    "        knn = neighbors.KNeighborsClassifier(n_neighbors=k[j])\n",
    "        # Fit KNN classifier using training set\n",
    "        knn = knn.fit(X_train, y_train)\n",
    "        \n",
    "        # 5) Extract test score for k[j]\n",
    "        scr[j, i] = knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to pandas dataframe\n",
    "tickers = shsPr.columns\n",
    "scr = pd.DataFrame(scr, index=k, columns=tickers[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & Analysis\n",
    "\n",
    "Now let's see the results in an overview. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr.max().nlargest(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following finance theory, returns should be distributed symmetrically. Thus the simplest guess would be to expect a share price to increase on 50% of the days and to decrease on the remaining 50%. Similar to guessing a coin flip, if we would guess an 'up' movement for every day, we obviously would - in the long run - be correct 50% of the times. This would make for a score of 50%. \n",
    "\n",
    "Looking in that light at the above summary, we see some very interesting results. For 10 out of 20 stocks KNN produces test scores of > 50% for even the 0.25th percentile. Let's plot the ones with the highest test-scores (ABBN, ZURN, NOVN, SIK) to see at what value of $k$ the best test-score is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms = ['ABBN', 'ZURN', 'NOVN', 'SIK']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in nms:\n",
    "    scr[col].plot(legend=True)\n",
    "plt.axhline(0.50, c='k', ls='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Zurich the peak is early (max. score around $k=60$) while the others peak later, i.e. with higher values of $k$. Furthermore, it seems interesting that for $k > 40$ test scores remained (barely) above 50%. If this is indeed a pattern we would have found a trading strategy, wouldn't we? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further assess our results we look into KNN's prediction of Givaudan's stock movements. For this we rerun our KNN classifier algorithm for ABBN as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create matrix with feature values of stock i\n",
    "X = pd.concat([Lag1['ABBN'], Lag2['ABBN'], smi], axis=1)\n",
    "X = X[3:]  # Drop first three rows with NaN (due to lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Remove first three rows of response dataframe\n",
    "#    to have equal no. of rows for features and response\n",
    "y = direction['ABBN']\n",
    "y = y[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Split data into training set...\n",
    "X_train = X[:'2016-06-30']\n",
    "y_train = y[:'2016-06-30']\n",
    "# ...and test set.\n",
    "X_test = X['2016-07-01':]\n",
    "y_test = y['2016-07-01':]\n",
    "\n",
    "# Convert responses to 1xN array (with .ravel() function)\n",
    "y_train = y_train.values.ravel()\n",
    "y_test  = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GIVN the maximum score is reached where $k=145$. You can check this with the `scr['ABBN'].idxmax()` command, which provides the index of the maximum value of the selected column. In our case, the index is equivalent to the value of $k$. Thus we run KNN with $k=145$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr['ABBN'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Run KNN\n",
    "# Instantiate KNN class for GIVN with k=145\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=145)\n",
    "# Fit KNN classifier using training set\n",
    "knn = knn.fit(X_train, y_train)\n",
    "\n",
    "# 5) Extract test score for ABB\n",
    "scr_ABBN = knn.score(X_test, y_test)\n",
    "scr_ABBN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score of 59.68% is the very same as what we have seen in above summary statistic. Nothing new so far. (Recall that the score is the total of correctly predicted outcomes.)\n",
    "\n",
    "However, the alert reader should by now raise some questions regarding our assumption that 50% of the returns should have been positive. In the long run, this might be true. But our training sample contained only 1'017 records and of these 534 were positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of 'up' days in training set\n",
    "y_train.sum() / y_train.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, if we would guess 'up' for every day of our test set and **given the distribution of classes in the test set is exactly as in our training set**, then we would predict the correct movement in 52.51% of the cases. So in that light, the predictive power of our KNN algorithm has to be put in perspective to the 52.51%.\n",
    "\n",
    "In summary, our KNN algorithm has a score of 59.68%. Our best guess (based on the training set) would yield a score of 52.51%. This still displays that overall our KNN algorithm outperforms our best guess. Nonetheless, the margin is smaller than initially thought. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "There are more tools to assess the accuracy of an algorithm. We postpone the discussion of these tools to a later chapter and at this stage restrict ourselves to the discussion of a tool called \"confusion matrix\". \n",
    "\n",
    "A confusion matrix is a convenient way of displaying how our classifier performs. In binary classification (with e.g. response $y \\in \\{0, 1\\}$) there are four prediction categories possible (Ting (2011)):\n",
    "\n",
    "* **True positive**: True response value is 1, predicted value is 1 (\"hit\")\n",
    "* **True negative**: True response value is 0, predicted value is 0 (\"correct rejection\")\n",
    "* **False positive**: True response value is 0, predicted value is 1 (\"False alarm\", Type 1 error)\n",
    "* **False negative**: True response value is 1, predicted value is 0 (\"Miss\", Type 2 error)\n",
    "\n",
    "These information help us to understand how our (KNN) algorithm performed. There are different two ways of arranging confusion matrix. James et al. (2013) follow the convention that column labels indicate the true class label and rows the predicted response class. Others have it transposed such that column labels indicate predicted classes and row labels show true values. We will use the latter approach as it is more common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/0207_ConfusionMatrixExplained.png\" alt=\"ConfusionMatrixExplained\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this in Python, we first predict the response value for each data entry in our test matrix `X_test`. Then we arrange the data in a suitable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 'up' (=1) or 'down' (=0) for test set\n",
    "pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data in DataFrame\n",
    "cfm = pd.DataFrame({'True direction': y_test,\n",
    "                    'Predicted direction': pred})\n",
    "cfm.replace(to_replace={0:'Down', 1:'Up'}, inplace=True)\n",
    "\n",
    "# Arrange data to confusion matrix\n",
    "print(cfm.groupby(['Predicted direction','True direction']) \\\n",
    "      .size().unstack('Predicted direction'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, rows represent the true outcome and columns show what class KNN predicted. In 31 cases, the test set's true response was 'down' (in our case represented by 0) and KNN correctly predicted 'down'. 120 times KNN was correct in predicting an 'up' (=1) movement. 19 returns in the test set were positive but KNN predicted a negative return. And in 83 out of 253 cases KNN predicted an 'up' movement whereas in reality the stock price decreased. The KNN score of 59.68% for ABB is the sum of true positive and negative (31 + 120) in relation to the total number of predictions (253 = 31 + 19 + 83 + 120). The error rate is 1 - score or (19 + 83)/253.\n",
    "\n",
    "Class-specific performance is also helpful to better understand results. The related terms are **sensitivity** and **specifity**. In the above case, sensitivity is the percentage of true 'up' movements that are identified. A good 86.3% (= 120 / (19 + 120)). The specifity is the percentage of 'down' movements that are correctly identified, here a poor 27.2% (= 31 / (31 + 83)). More on this in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because confusion matrices are important to analyze results, `Scikit-learn` has its own [command to generate it](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). It is part of the `metrics` sublibrary. The difficulty is that in contrast to above (manually generated) table, the function's output provides no labels. Therefore one must be sure to know which value are where. Here's the code to generate the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general it is tremendously helpful to visualize results. At the time of writing in February 2022, Anaconda shipped with `Sklearn` version 0.24.2. This `Sklearn` version uses a confusion matrix-plotting function that is deprecated in versioun 1.0. Therefore, be aware that below plotting function only works for versions < 1.0. Details see [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html#sklearn.metrics.plot_confusion_matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# To plot the normalized figures, add normalize = 'true', 'pred', or 'all'\n",
    "plot_confusion_matrix(estimator = knn, \n",
    "                      X = X_test, y_true = y_test,\n",
    "                      display_labels = ['Down', 'Up'],\n",
    "                      values_format = '.0f');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the normalized figures, add normalize = 'true', 'pred', or 'all'\n",
    "# This time with different color scheme\n",
    "plot_confusion_matrix(estimator = knn, \n",
    "                      X = X_test, y_true = y_test,\n",
    "                      cmap = plt.cm.Blues,\n",
    "                      normalize = 'all',\n",
    "                      display_labels = ['Down', 'Up'],\n",
    "                      values_format = '.2f');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Ressources\n",
    "\n",
    "\n",
    "In writing this notebook, many ressources were consulted. For internet ressources the links are provided within the textflow above and will therefore not be listed again. Beyond these links, the following ressources were consulted and are recommended as further reading on the discussed topics:\n",
    "\n",
    "* Batista, Gustavo, and Diego Furtado Silva, 2009, How k-nearest neighbor parameters affect its performance, in *Argentine Symposium on Artificial Intelligence*, 1–12, sn.\n",
    "* Fortmann-Roe, Scott, 2012, Understanding the Bias-Variance Tradeoff from website, http://scott.fortmann-roe.com/docs/BiasVariance.html, 08/15/17.\n",
    "* Guggenbuehler, Jan P., 2015, Predicting net new money using machine learning algorithms and newspaper articles, Technical report, University of Zurich, Zurich.\n",
    "* James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2013, *An Introduction to Statistical Learning: With Applications in R* (Springer Science & Business Media, New York, NY).\n",
    "* Müller, Andreas C., and Sarah Guido, 2017, *Introduction to Machine Learning with Python* (O’Reilly Media, Sebastopol, CA).\n",
    "* Russell, Stuart, and Peter Norvig, 2009, *Artificial Intelligence: A Modern Approach* (Prentice Hall Press, Upper Saddle River, NJ).\n",
    "* Ting, Kai Ming, 2011, Confusion matrix, in Claude Sammut, and Geoffrey I. Webb, eds., *Encyclopedia of Machine Learning* (Springer Science & Business Media, New York, NY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.show_versions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
